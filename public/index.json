[{"content":"Introduction As you might be well aware of Big Data isn\u0026rsquo;t going anywhere anytime soon. Big Data Analytics, an active research area which is concerned with analyzing large scale, fast paced and varying types of data will stay a challenging field.\nYou might ask yourself - how this is linked to Principal Component Analysis? This statistical analysis is able to deal with one characteristic trait of Big Data. As you may recall Big Data is primarly characterized by four Vs (Volume, Variety, Velocity, Veracity).\nIf we take a look at the first trait we can think of Volume as the amount of sample points in a datasets as well as the number of features a dataset has to offer. If we have we work with a vast amount of variables we are talking about high dimensional data:\nThe following problems will most likely occur when you work with high dimensional data:\nMany features are intercorrelated with one another and many ML algorithmns can\u0026rsquo;t work under this condition It\u0026rsquo;s hard to identify dependant variables with high predictive power Visualizing data which has more than three dimensions isn\u0026rsquo;t possible As you maybe guessed already, a Principal Component Analysis is able to adress those problems.\nWhat is Principal Component Analysis (PCA)? But what is Principal Component Analysis? PCA is a dimensional reduction technique which aims to reduce the number of variables while maintaining as much information about them as possible. PCA is an method of Feature Extraction which means k variables get compressed to n variables (altough n \u0026lt; k). Those newly created variables are called principal components, hence the name of the analysis. Ideally only a few principal components are able to explain the lion share of the variance in the original data.\nAs mentioned PCA extracts features and is thus different from Feature Elimination which is another type of dimensionality reduction. In contrast to Feature Elimination where dependant variables get elimated based on a certain criteria (e.g. when the correlation with the target variable is below a threshold value) the PCA retains all variables and only recomposes them. A great benefit is that all principal components are independent from each other and have zero intercorelation with one another. This makes modeling with linear type models a lot easier.\nWhen can PCA be used? Ask yourself this three questions if you consider using PCA:\nDo you want to reduce the number of variables, but aren’t able to identify variables to completely remove from consideration? Do you want to ensure your variables are independent of one another? Are you comfortable making your independent variables less interpretable? If the answer to all questions is yes an PCA is probably a good idea. Before you apply PCA you should check if alle the formal requirements are met. Those requirements are:\nThe relationship between variables should be linear There shouldn\u0026rsquo;t be any outlier The variables should be continuus The larger the sample size the more precise the result (n \u0026gt; 20 for each variable) An practial example The Dataset We will be working with the Boston house prices dataset which is a data source which has been used in many Machine learning papers and is usually used for regression problems. But we won\u0026rsquo;t build a prediction model here. We will be using the packages pandas, numpy, matplotlib and sklearn which are popular and user friendly modules for Data Science and Machine Learning.\n# import packages from sklearn.datasets import load_boston from sklearn.preprocessing import StandardScaler from factor_analyzer.factor_analyzer import calculate_kmo from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity import pandas as pd import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D # 3D Scatterplot # load dataset boston = load_boston() df = pd.DataFrame(boston.data) cols = boston.feature_names df.columns = cols df['MEDV'] = boston.target df.head(5) CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT MEDV 0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.98 24.0 1 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.14 21.6 2 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.03 34.7 3 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.94 33.4 4 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33 36.2 Exploratory Data Analysis df.info() \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 506 entries, 0 to 505 Data columns (total 14 columns): CRIM 506 non-null float64 ZN 506 non-null float64 INDUS 506 non-null float64 CHAS 506 non-null float64 NOX 506 non-null float64 RM 506 non-null float64 AGE 506 non-null float64 DIS 506 non-null float64 RAD 506 non-null float64 TAX 506 non-null float64 PTRATIO 506 non-null float64 B 506 non-null float64 LSTAT 506 non-null float64 MEDV 506 non-null float64 dtypes: float64(14) memory usage: 55.5 KB As we can see clearly from the df.info() summary method we have 14 columns and 506 sample points. The MEDV column which is the median value of oner-occupied homes in $1000 steps is typically the target variable. All columns are formated as continuus variables. You could argue that a PCA doesn\u0026rsquo;t make to much sense when you have only 13 dependant variables. For the sake of this example we will do it anyway.\nSince we imported the dataset via an dedicated machine learning library called sklearn we have access to an detailed description method which gives us additional information about the dataset and its variables.\ndescription = boston.DESCR.split('\\n')[0:29] # only extract dataset characteristics description = '\\n'.join(description) print(description) .. _boston_dataset: Boston house prices dataset --------------------------- **Data Set Characteristics:** :Number of Instances: 506 :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target. :Attribute Information (in order): - CRIM per capita crime rate by town - ZN proportion of residential land zoned for lots over 25,000 sq.ft. - INDUS proportion of non-retail business acres per town - CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) - NOX nitric oxides concentration (parts per 10 million) - RM average number of rooms per dwelling - AGE proportion of owner-occupied units built prior to 1940 - DIS weighted distances to five Boston employment centres - RAD index of accessibility to radial highways - TAX full-value property-tax rate per $10,000 - PTRATIO pupil-teacher ratio by town - B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town - LSTAT % lower status of the population - MEDV Median value of owner-occupied homes in $1000's :Missing Attribute Values: None There are several categories of variables in this dataset. We can see socioeconomic, educational, environmental and property related variables. The hint that there are no missing values is helpful since we usually had to check this for ourselves. In general the absence of missing values makes working with data a lot easier.\nAfter doing a quick exploration of the data, let\u0026rsquo;s finally apply the PCA. Here are the steps necessary to apply the Principal Component Analysis to this dataset. The general layout of those steps are the following:\nSeperate dependant and independant* variables Standardize the features Suitability Tests Calculate Covariance Matrix Apply Matrix decompensation (Eigendecomposition) Compute and evaluate Explained Variance Construct Projection Matrix Visualize dataset Seperate dependant and independant variables We start by seperating predictor and target variables since the PCA should be applied to the dependant variables only.\nX, y = df.iloc[:, 0:13], df.iloc[:, 13] X.head(5) # dependant variables CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT 0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.98 1 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.14 2 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.03 3 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.94 4 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33 Standardize features import warnings warnings.simplefilter(\u0026quot;ignore\u0026quot;, UserWarning) fig = plt.figure(figsize = (20,12)) ax = fig.gca() df.hist(ax = ax); As we can see above the scale varies quite a lot between the different variables. This makes a Rescaling necessary for the PCA to work properly. We are using the the StandardScaler from the sklearn.preprocessing module to transform the data onto unit scale (mean = 0, variance = 1).\nX_std = StandardScaler().fit_transform(X) Suitability Tests Before we can apply the necessary steps of a PCA we have to perform two statistical tests which measure if a PCA is suitable for the present dataset, the Bartlett and Kaiser-Meyer-Olkin test.\nBartlett\u0026rsquo;s criterion Bartlett’s test of sphericity checks the null hypothesis that the correlation matrix is equal to the identity matrix. The identiy matrix is a n × n square unit matrix with ones on the main diagonal and zeros elsewhere. The correlation matrix is a table showing correlation coefficients between variables. The correlation coefficient measure the linear relationship between two variables. The hypothesis should be rejected, so we can continue with the PCA.\n# Test Hypothesis chi_square_value,p_value=calculate_bartlett_sphericity(df) print('Chi Square Value: \\n%s' %chi_square_value) print('p-Value: \\n%s' %p_value) The p-value of the Bartlett test is zero and therefore statistically significant. We can conclude that the correlation matrix is not equal to the identity matrix and proceed with the next test.\nKaiser-Meyer-Olkin criterion The Kaiser-Meyer-Olkin (KMO) test also measures the suitability of a dataset for Principal component analysis. The minimal overall value for all features should be above 0.6.\n# Calculate KMO kmo_all, kmo_model = calculate_kmo(X_std) print('Overall KMO: \\n%s' %kmo_model) The overall value is 0.85 which is a fantastic value. This value indicates that we can proceed with the next steps.\nCreate Covariance Matrix The covariance matrix is a measure which measures how variables are associated with one another. The covariance between two variables can be computed by this formular:\n$$ \\sigma_{j k}=\\frac{1}{n-1} \\sum_{i=1}^{n}\\left(x_{i j}-\\bar{x}{j}\\right)\\left(x{i k}-\\bar{x}_{k}\\right) $$\nWe calculate the Covariance Matrix because the Eigendecomposition which is applied in the classic method of a PCA requires this matrix.\ncov_mat = np.cov(X_std.T) print('Covariance matrix: \\n%s' %cov_mat[0:4]) # only a sample of features Apply Matrix decompensation (Eigendecomposition) Eigendecomposition of a matrix is a type of decomposition that involves decomposing a square matrix into a set of eigenvectors and eigenvalues. We use Singular Value Decomposition (SVD) which is the fastest approach to do a Matrix decomposition.\nEigenvectors are unit vectors, which means that their length or magnitude is equal to 1.0. Eigenvectors are column vectors with shape (1, n). Eigenvalues are coefficients applied to eigenvectors that give the vectors their length or magnitude.\neig_vecs, eig_vals, _ = np.linalg.svd(cov_mat) print('Eigenvectors \\n%s' %eig_vecs[0:4]) print('\\nEigenvalues \\n%s' %eig_vals) Eigenvectors [[-0.2509514 -0.31525237 0.24656649 0.06177071 0.08215692 -0.21965961 0.77760721 -0.15335048 0.26039028 -0.01936913 0.10964435 0.08676107 -0.0459523 ] [ 0.25631454 -0.3233129 0.29585782 0.12871159 0.32061699 -0.3233881 -0.27499628 0.40268031 0.35813749 -0.26752723 -0.26275629 -0.07142528 0.08091897] [-0.34667207 0.11249291 -0.01594592 0.01714571 -0.00781119 -0.0761379 -0.33957645 -0.17393172 0.64441615 0.36353226 0.30316943 -0.11319963 0.25107654] [-0.00504243 0.45482914 0.28978082 0.81594136 0.08653094 0.16749014 0.07413621 0.02466215 -0.01372777 0.00618184 -0.01392667 -0.00398268 -0.03592171]] Eigenvalues [6.1389812 1.43611329 1.2450773 0.85927328 0.83646904 0.65870897 0.5364162 0.39688167 0.27749173 0.22067394 0.18638271 0.16963823 0.06363502] Compute and evaluate Explained Variance The eigenvectors only define the direction or magnitude of an axis since their unit length is normed to a length of 1. In order to decide which eigenvectors should be dropped when creating a lower dimensional subspace we have to evaluate the eigenvalues. The explained variance is a measurement based on these eigenvalues which can tell us how much information can be attributed to each principal component.\n# Calculate individual and cumulated variance tot_var = sum(eig_vals) exp_var = (eig_vals / tot_var)*100 cum_exp_var = np.cumsum(exp_var) # Plot variance to graph plt.figure(figsize=(8, 6)) plt.bar(range(len(exp_var)), exp_var, label='individual explained variance') plt.step(range(len(exp_var)), cum_exp_var, where='mid', label='cumulative explained variance') plt.ylabel('Explained variance ratio') plt.xlabel('Principal components') plt.legend(loc='center') plt.tight_layout(); The essential question now is: How do we choose the number of principal components which should represent our dataset? There are basically four viable methods for picking the number of components:\nMethod 1: We select how many dimensions we want to keep based on our analysis goal. Perhaps we want to visually represent data in two dimensions which would mean we keep two principal components Method 2: Calculate the cumulative explained variance (as in the graph above) and select as many principal components until a defined threshold is reached (typical would be a value of 80%). Method 3: Calculate the individual explained variance for each component and look for a significant drop in explained variance. Then pick alle components before the big drop. This approach is also called \u0026lsquo;find the elbow\u0026rsquo;. Method 4: Compute the individual explained variance and only pick those princripal components which explain at least 10% of the total variance Projection Matrix Finally we will construct the Projection Matrix W which we will use to transform the orginal dataset to a lower dimensional space. In this our case i will choose the three principal components as basis for the projection matrix since i want to create a three dimensional visualization of our dataset (Method 1).\n# Pick first three eigenvectors matrix_w = eig_vecs[:, 0:3] print(f'\\nShape of Projection Matrix: {matrix_w.shape}') print('\\nProjection Matrix: \\n%s' %matrix_w) With help of this Projection Matrix we can create the lower dimensional dataset which contains new values for each observation. As you can see the new dataset only has three dependant variables: PC1, PC2, PC3. I created three new values (cheap, moderate, expensive) for the target variable MEDV which represents the price of a house and will be used for coloring.\n# Create new feature space with principal components result = pd.DataFrame(X_std.dot(matrix_w)) result.columns = ['PC1', 'PC2', 'PC3'] result = pd.concat([result, pd.cut(y, bins=[0, 15, 30, 50], labels=['cheap', 'moderate', 'expensive'])], axis=1) result.head(10) PC1 PC2 PC3 MEDV 0 2.098297 0.773113 0.342943 moderate 1 1.457252 0.591985 -0.695199 moderate 2 2.074598 0.599639 0.167122 expensive 3 2.611504 -0.006871 -0.100284 expensive 4 2.458185 0.097712 -0.075348 expensive 5 2.214852 -0.009487 -0.672381 moderate 6 1.358881 0.349872 -0.371999 moderate 7 0.842045 0.577800 -0.518540 moderate 8 0.179928 0.342518 -1.349639 moderate 9 1.074184 0.316201 -0.558469 moderate Visualize dataset Finally let\u0026rsquo;s plot the principal components in a three dimensional space and use the recoded target variable for colouring.\n# Plot initialisation fig = plt.figure(figsize=(12,8)) ax = fig.add_subplot(111, projection='3d') ax.scatter(result['PC1'], result['PC2'], result['PC3'], c=result['MEDV'].cat.codes, cmap=\u0026quot;Set2_r\u0026quot;, s=60) # Make simple, bare axis lines through space (red): xAxisLine = ((min(result['PC1']), max(result['PC1'])), (0, 0), (0,0)) ax.plot(xAxisLine[0], xAxisLine[1], xAxisLine[2], 'r') yAxisLine = ((0, 0), (min(result['PC2']), max(result['PC2'])), (0,0)) ax.plot(yAxisLine[0], yAxisLine[1], yAxisLine[2], 'r') zAxisLine = ((0, 0), (0,0), (min(result['PC3']), max(result['PC3']))) ax.plot(zAxisLine[0], zAxisLine[1], zAxisLine[2], 'r') # Label the axes, title and legend ax.set_xlabel(\u0026quot;PC1\u0026quot;) ax.set_ylabel(\u0026quot;PC2\u0026quot;) ax.set_zlabel(\u0026quot;PC3\u0026quot;) ax.set_title(\u0026quot;Boston Housing Prices - PCA results\u0026quot;); Summary Let\u0026rsquo;s summarize what what have learned about PCA:\nPCA is used to overcome features redundancy in a data set. These features are low dimensional in nature. These features a.k.a components are a resultant of normalized linear combination of original predictor variables. These components aim to capture as much information as possible with high explained variance. The first component has the highest variance followed by second, third and so on. Normalizing data becomes extremely important when the predictors are measured in different units. PCA works best on data set having 3 or higher dimensions. Because, with higher dimensions, it becomes increasingly difficult to make interpretations from the - resultant cloud of data. PCA is applied on a data set with numeric variables. PCA is a tool which helps to produce better visualizations of high dimensional data. ","permalink":"https://example.com/blog/introduction-to-principal-component-analysis/","title":"Introduction to Principal Component Analysis"},{"content":"Introduction In real-world datasets it is often the case that you have a mixed variable types. While Machine learning algorithmns usually can handle only numerical data, they can\u0026rsquo;t work with categorical variables. Being able to use categorical variables in modeling makes it necessary to transform those variables. This process is called Encoding and there are many different strategies for it.\nThere are two types of variables which can be seen as categorical variables:\nNominal variables: Nominal data is made of discrete values with no numerical relationship between the different values — mean and median are meaningless. An example here would be the colour of a car or the job title of a person.\nOrdinal variables: A variable used to rank a sample of individuals with respect to some characteristics, but differences (i.e., intervals) and different points of the scale are not necessarily equivalent. An example here would be the\nBasic Encoding Strategies The following encoding strategies are easy to understand and very popular in Machine Learning:\nOne Hot Encoding Ordinal Encoding Binary Encoding Frequency Encoding Hashing Encoding Sum Encoding Mean Encoding Leave One Out Encoding To illustrate these different encoding strategies i will be using an sample dataset which i manuelly created in Excel. You can find it in this Github Repository.\nOkay let\u0026rsquo;s begin by importing all necessary python packages. If your working with the Anaconda Distribution all packages except for the category_encoders module should be preinstalled. Installation instructions for this package can be found here.\n# import packages import os import numpy as np import pandas as pd import category_encoders as ce # load dataset root_dir = os.path.abspath(os.pardir) data_dir = os.path.join(root_dir, 'data') df = pd.read_excel(os.path.join(data_dir, 'dummy_dataset.xlsx')) # seperate predictor and target variable X = df.loc[:, ['id', 'age', 'iq', 'hair']] y = df.loc[:, ['target']] Let\u0026rsquo;s have a look at the dataset:\ndf.head(10) id age iq hair target 0 1 0-17 very low brown 0 1 2 18-29 low black 1 2 3 30-49 medium blonde 0 3 4 50-69 high black 1 4 5 70+ very high blonde 0 5 6 18-29 medium brown 1 6 7 30-49 high blonde 1 7 8 30-49 low brown 1 8 9 50-69 medium brown 0 9 10 18-29 high black 0 One Hot Encoding I\u0026rsquo;ll mention this technique first since it is the go-to approach to encode data and very easy to understand. This approach is also called dummy or indicator encoding. When transforming a variable via One Hot Encoding every unique value of the original columns get it\u0026rsquo;s own column. This means the number of columns in a dataframe gets increased by k-1 (where k is the number of unique values). This type of encoding can be applied to nominal as well as categorical variables.\none_hot_enc = ce.OneHotEncoder(cols=['hair'], use_cat_names=True) encoding = one_hot_enc.fit_transform(X['hair'], y) combined = pd.concat([X['hair'], encoding], axis=1) combined.sort_values('hair') hair hair_brown hair_black hair_blonde 1 black 0 1 0 3 black 0 1 0 9 black 0 1 0 2 blonde 0 0 1 4 blonde 0 0 1 6 blonde 0 0 1 0 brown 1 0 0 5 brown 1 0 0 7 brown 1 0 0 8 brown 1 0 0 As you can see each hair color gets it\u0026rsquo;s own column. Since a person can only have one hair color there is only a single 1 in each row. As you might also imagine the number of columns created by this approach can very large and therefore computionally and memory expensive especially if we deal with high cardinality features. High cardinality features are variables which have a lot of unique values. An good example here are zip codes.\nOrdinal Encoding This technique as the name suggests can only be applied to ordinal features. Here each string will be replaced by an corresponding integer. This replacement make sense and is recommended since there is an natural order in ordinal variables. Furthermore this approach is very cost effective because no additional columns are created.\nordinal_enc = ce.OrdinalEncoder(cols=['iq'], mapping=[{'col':'iq', 'mapping': {'very low': 1, 'low': 2, 'medium': 3, 'high': 4, 'very high': 5}}]) encoding = ordinal_enc.fit_transform(X['iq'], y).rename(columns={'iq':'iq_enc'}) combined = pd.concat([X['iq'], encoding], axis=1) combined.sort_values('iq_enc') iq iq_enc 0 very low 1 1 low 2 7 low 2 2 medium 3 5 medium 3 8 medium 3 3 high 4 6 high 4 9 high 4 4 very high 5 The following things happend when applying the encoding:\nThe value \u0026lsquo;very low\u0026rsquo; got replaced by a 1, the value low got replaced by a 2 and so fourth The natural order has been preserved because of the mapping argument which the encoder has been given. Otherwise the assignment would have been random. Binary Encoding This encoding method can be seen as hybrid between One Hot and Hashing Encoders. It creates fewer features as the One Hot Encoding approach while preserving a more unique character of values. It works very well with high dimensional ordinal data altough this combination is very rare.\nbinary_enc = ce.BinaryEncoder(cols=['hair']) encoding = binary_enc.fit_transform(X['hair'], y) combined = pd.concat([X['hair'], encoding], axis=1) combined.sort_values('hair') hair hair_0 hair_1 hair_2 1 black 0 1 0 3 black 0 1 0 9 black 0 1 0 2 blonde 0 1 1 4 blonde 0 1 1 6 blonde 0 1 1 0 brown 0 0 1 5 brown 0 0 1 7 brown 0 0 1 8 brown 0 0 1 Altough it first might seem like this technique is exactly like One Hot Encoding, the appearances are deceptive. If you observe the third row (index = 2) you will notice that there are two \u0026lsquo;1s\u0026rsquo; in the whole row. This wouldn\u0026rsquo;t be possible in a One Hot Encoding. This row tells us that hair color of the respondent is blond. Blond is the third type of hair colour and three is represented by 011 in the binary language.\nBinary Encoding follow these steps:\nThe categories are encoded by an Ordinal Encoder if they aren’t already in numeric form Then those integers are converted into binary code, so for example 5 becomes 101 and 8 becomes 112. Then the digits from that binary string are split into separate columns. So if there are 4–7 values in an ordinal column then 3 new columns are created: one for the first bit, one for the second, and one for the third. Each observation is encoded across the columns in its binary form. Frequency Encoding This encoding approach is rather uncommon and can be used for nominal as well as ordinal features. In this case the values get replaced by their frequency in relation to the whole dataset, hence the name. In this case we won\u0026rsquo;t use the category_encoders package. Instead we\u0026rsquo;ll use panda methods to encode the data.\nfreq_enc = df.groupby('hair').size() / len(df) df.loc[:, 'hair_enc'] = df['hair'].map(freq_enc) df.loc[:, ['hair', 'hair_enc']].sort_values('hair').sort_values('hair_enc') hair hair_enc 1 black 0.3 3 black 0.3 9 black 0.3 2 blonde 0.3 4 blonde 0.3 6 blonde 0.3 0 brown 0.4 5 brown 0.4 7 brown 0.4 8 brown 0.4 Since the color brown is the most frequent value in the hair column is has the highest value in the encoded columns followed by the other two values. Instead of relative value we could also use absolute frequencies (counts).\nHashing Encoding This technique implements the hashing trick. It is similar to one-hot encoding but with less newly created columns and some loss in information because of collision effects. The collisions do not significantly affect performance unless there is a great deal of overlap. An detailed discussion of this method can be found here and an in-depth explanation can be found in this article.\nhash_enc = ce.HashingEncoder(cols=['hair']) encoding = hash_enc.fit_transform(X['hair'], y) combined = pd.concat([X['hair'], encoding], axis=1) combined.sort_values('hair') hair col_0 col_1 col_2 col_3 col_4 col_5 col_6 col_7 1 black 0 1 0 0 0 0 0 0 3 black 0 1 0 0 0 0 0 0 9 black 0 1 0 0 0 0 0 0 2 blonde 0 1 0 0 0 0 0 0 4 blonde 0 1 0 0 0 0 0 0 6 blonde 0 1 0 0 0 0 0 0 0 brown 0 0 0 1 0 0 0 0 5 brown 0 0 0 1 0 0 0 0 7 brown 0 0 0 1 0 0 0 0 8 brown 0 0 0 1 0 0 0 0 Since the number of dimensions defaults to 8, the same amount of columns get created by the encoding. Since there are only three unique values in the hair column we see five columns with only zeros. Hashing gets interesing when we have a lot of unique values and the number of newly added columns should be smaller than the amount of unique values. In Kaggle competitions hashing has been a very sucessfull method for encoding high cardinality features.\nSum Encoding A Sum Encoder compares the mean of the dependent variable (target) for a given level of a categorical column to the overall mean of the target. This method is very similar to One Hot Encoding except that the number of created columns is always one less. This is the case because one unique value is always held constant.\nsum_enc = ce.SumEncoder(cols=['age']) encoding = sum_enc.fit_transform(X['age'], y) combined = pd.concat([X['age'], encoding, y], axis=1) combined.sort_values('age') age intercept age_0 age_1 age_2 age_3 target 0 0-17 1 1.0 0.0 0.0 0.0 0 1 18-29 1 0.0 1.0 0.0 0.0 1 5 18-29 1 0.0 1.0 0.0 0.0 1 9 18-29 1 0.0 1.0 0.0 0.0 0 2 30-49 1 0.0 0.0 1.0 0.0 0 6 30-49 1 0.0 0.0 1.0 0.0 1 7 30-49 1 0.0 0.0 1.0 0.0 1 3 50-69 1 0.0 0.0 0.0 1.0 1 8 50-69 1 0.0 0.0 0.0 1.0 0 4 70+ 1 -1.0 -1.0 -1.0 -1.0 0 That one value is always contant can be observed in row 5 (index = 4), where the value for 70+ is always encoded as -1 regardless of the column. Sum Encoding is is commonly used in Linear Regression (LR) types of models.\nTarget Encoding Target Encoding or Mean Encoding directly correlates the encoded variable with a discrete target variable. So this approach can only be used in classification problems. The danger of this method lies within the problem of overfitting which can only be addressed by regularization. Nevertheless this Encoding technique has been very sucessfully used in Kaggle competitions.\ntarget_enc = ce.TargetEncoder(cols=['hair']) encoding = target_enc.fit_transform(X['hair'], y).rename(columns={'hair':'hair_enc'}) combined = pd.concat([X['hair'], encoding, y], axis=1) combined.sort_values('hair') hair hair_enc target 1 black 0.6468 1 3 black 0.6468 1 9 black 0.6468 0 2 blonde 0.3532 0 4 blonde 0.3532 0 6 blonde 0.3532 1 0 brown 0.5000 0 5 brown 0.5000 1 7 brown 0.5000 1 8 brown 0.5000 0 As you can see there are only three different numeric values. Reason for this is that like in Frequence Encoding each unique category gets a new numerical value between 0 to 1. In the first row we can see that observations with the hair color brown are in 50% of the cases correlated to the target variable, hence the value 0.5.\nLeave One Out Encoding Leave-one-out Encoding (LOO or LOOE) is another example of a target-based encoder. The name speaks for itself: we compute the mean target of category k for observation i if observation i would be removed from the dataset.\nloo = ce.LeaveOneOutEncoder(cols=['hair']) encoding = loo.fit_transform(X['hair'], y).rename(columns={'hair':'hair_enc'}) combined = pd.concat([X['hair'], encoding, y], axis=1) combined.sort_values('hair') hair hair_enc target 1 black 0.500000 1 3 black 0.500000 1 9 black 1.000000 0 2 blonde 0.500000 0 4 blonde 0.500000 0 6 blonde 0.000000 1 0 brown 0.666667 0 5 brown 0.333333 1 7 brown 0.333333 1 8 brown 0.666667 0 There are other more complicated Target based Encoder like the M-Estimate, Weight of Evidence or James-Steiner Encoder (WOE) which also had their share of sucess in Kaggle competitions. If you\u0026rsquo;re interested in those, check out this blog post on Towards Data Science.\nSummary Here a quick summary of all presented encoding techniques in a tabular format. This should help you and me to maintain a clear overview:\nTechnique What does it do? Variable Type One Hot Encoding Each category gets its own column Nominal, Ordinal Ordinal Encoding Each category gets mapped to an integer value Ordinal Binary Encoding Each category gets mapped to a binary code and columns split according to the code length. Ordinal Frequency Encoding The frequency of each value in category gets calculated and mapped Nominal, Ordinal Hashing Encoding Replace values by hash code with varying length and columns split according to code length Nominal, Ordinal Sum Encoding Similar to One Hot Encoding, except one less column created Nominal, Ordinal Target Encoding Correlate variable to target variable Nominal, Ordinal Leave One Out Encoding Similar to Target Encoding except current observation gets ignored in calculation Nominal, Ordinal Final Discussion Okay let\u0026rsquo;s wrap this up by reflecting on the decision process when choosing an encoding method.\nRelevant questions to ask when you think about which Encoding strategy to choose are:\nWhat variable type do i have? Ordinal or Nominal How many unique values does the variable have (cardinality)? Low, Medium or High What type of Machine Learning problem do i try to solve? For example: Classification, Regression, Clustering I can give you the following advise when choosing an technique to encode your data:\nFirst check the type of ML problem: Any kind of Target Encoding only works for classification In general the following Encodings make sense for ordinal features: Ordinary, Binary, OneHot, Leave One Out, Target Encoding If you have a ordinal columns with a lot of features (rare case) take a Binary Encoder. Leave One Out or Target Encoding also make sense. In general the following Encodings make sense for nominal features: OneHot, Hashing, LeaveOneOut, and Target encoding . Avoid OneHot for high cardinality columns If you need a visual map to guide you through the decision, here is a flow chart i found:\nsource\n","permalink":"https://example.com/blog/dealing-with-categorical-data/","title":"Dealing with categorical data"}]