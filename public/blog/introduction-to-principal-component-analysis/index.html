<!doctype html><html lang=en><head><meta charset=utf-8><link rel="shortcut icon" href=/favicon.ico type=image/x-icon><meta name=viewport content="width=device-width,initial-scale=1"><title>Introduction to Principal Component Analysis - Marc-Philipp Esser | Personal Website</title><meta name=description content="Introduction to Principal Component Analysis"><meta name=keywords content="principal-component-analysis,pca,data-science,dimensionality-reduction,feature-extraction,data-preperation,python"><meta name=author content="Marc-Philipp Esser"><meta property="og:title" content="Introduction to Principal Component Analysis"><meta property="og:description" content="Introduction to Principal Component Analysis"><meta property="og:type" content="article"><meta property="og:url" content="https://example.com/blog/introduction-to-principal-component-analysis/"><meta property="article:section" content="blog"><meta property="article:published_time" content="2019-12-30T00:00:00+00:00"><meta property="article:modified_time" content="2019-12-30T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Introduction to Principal Component Analysis"><meta name=twitter:description content="Introduction to Principal Component Analysis"><link rel=stylesheet href=/style.min.5297c96c59a52afaa5bcda4a6cedf3813081f64025c209b25b2ee6d0c8f74d462b625ad3404a92a14d7a51b4ec0a420337ae70f426fa4bce2d5f7459a3ca7274.css integrity="sha512-UpfJbFmlKvqlvNpKbO3zgTCB9kAlwgmyWy7m0Mj3TUYrYlrTQEqSoU16UbTsCkIDN65w9Cb6S84tX3RZo8pydA=="><link rel=stylesheet href=/lib/css/prism.min.6226f06f992e0d6166b0e26724efd050dcc381202a752892ba523b1b865de2ea5e427f8f7d10de682fc35d6e7444018247d1f25db5e1e3bab17068ce191c5886.css integrity="sha512-Yibwb5kuDWFmsOJnJO/QUNzDgSAqdSiSulI7G4Zd4upeQn+PfRDeaC/DXW50RAGCR9HyXbXh47qxcGjOGRxYhg=="><script>localStorage.theme==="dark"||!("theme"in localStorage)&&window.matchMedia("(prefers-color-scheme: dark)").matches?document.documentElement.setAttribute("data-theme","dark"):document.documentElement.setAttribute("data-theme","light")</script><script defer src=/js/header.7a2a109ec3782c57bad0332b662f8a5f41765505936b69868eb8bd5241de9daf23c388e82ca1831f6d09935013dcb9f71bfa7face3975880c1076028b7b0a6e1.js integrity="sha512-eioQnsN4LFe60DMrZi+KX0F2VQWTa2mGjri9UkHena8jw4joLKGDH20Jk1AT3Ln3G/p/rOOXWIDBB2Aot7Cm4Q=="></script>
<script defer src=/js/zooming.684b5d075bf94d0adfa21a7e7eb9acec1ddfb2e7b47d6657981617f0db0cf50949f1172801595afa3051f51b28d67f6a2d0c41be677b59b564307d9dbe4a4fd2.js integrity="sha512-aEtdB1v5TQrfohp+frms7B3fsue0fWZXmBYX8NsM9QlJ8RcoAVla+jBR9Rso1n9qLQxBvmd7WbVkMH2dvkpP0g=="></script>
<script defer src=/js/prism.c16efe6b4f0ae7df5f934ad76ec45633d61b926e66c11dae8f2f56f5e6fc27ba412a3f88ba617f8436480ac928ca3fbc0c3cdee84992a64130d42ea551dad860.js integrity="sha512-wW7+a08K599fk0rXbsRWM9Ybkm5mwR2ujy9W9eb8J7pBKj+IumF/hDZICskoyj+8DDze6EmSpkEw1C6lUdrYYA==" data-manual></script>
<script defer src=/js/search-en.677e05c714f837051483647d736c45051126c22a2696ca5f981b59da962d48962f3ea71150f559ba84884fd1aadf1c7aadce55bb31a3e5b75c0f173b763c109a.js integrity="sha512-Z34FxxT4NwUUg2R9c2xFBREmwiomlspfmBtZ2pYtSJYvPqcRUPVZuoSIT9Gq3xx6rc5VuzGj5bdcDxc7djwQmg=="></script></head><body><main><header><div class=brand><div id=sidebar_btn><svg id="menu_icon" width="26" height="26" viewBox="0 0 24 24"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2.5" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></svg></div><div><a href=/>Marc-Philipp Esser</a></div></div><div class=toolbox><div id=theme_tool><svg id="dark_mode_btn" class="hidden toolbox-btn" width="18" height="18" viewBox="0 0 24 24"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2.5" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></svg><svg id="light_mode_btn" class="hidden toolbox-btn" width="18" height="18" viewBox="0 0 24 24"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg></svg></div><div id=search_tool><svg id="search_btn" class="toolbox-btn" width="18" height="18" viewBox="0 0 24 24"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2.5" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"><circle cx="11" cy="11" r="8"/><line x1="21" y1="21" x2="16.65" y2="16.65"/></svg></svg><div id=search_menu_wrapper class=hidden><div id=search_menu><div id=search_menu_toolbar><div id=search_menu_input_wrapper><input id=search_menu_input type=text placeholder='Search Posts'></div><div id=search_menu_close_btn><svg width="18" height="18" viewBox="0 0 24 24"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-x"><line x1="18" y1="6" x2="6" y2="18"/><line x1="6" y1="6" x2="18" y2="18"/></svg></svg></div></div><div id=search_menu_results></div></div></div></div></div></header><nav id=navbar class=pure-menu><ul class=pure-menu-list><li class="navbar-item pure-menu-item"><a href=/cv/ class=pure-menu-link>Curricum Vitae</a></li><li class="navbar-item pure-menu-item insection"><a href=/blog/ class=pure-menu-link>Blog</a></li><li class="navbar-item pure-menu-item"><a href=/project/ class=pure-menu-link>Projects</a></li></ul></nav><div id=sidebar_canvas_overlay class=hidden></div><div id=sidebar class=close><ul><li><a href=/cv/>Curricum Vitae</a></li><li><a href=/blog/>Blog</a></li><li><a href=/project/>Projects</a></li></ul></div><div id=content class=content-margin><div class=collapsible-menu-wrapper><div class=collapsible-menu-type><span>Table of contents</span></div><div class=collapsible-menu><nav id=TableOfContents><ul><li><a href=#the-dataset>The Dataset</a></li><li><a href=#exploratory-data-analysis>Exploratory Data Analysis</a></li><li><a href=#seperate-dependant-and-independant-variables>Seperate dependant and independant variables</a></li><li><a href=#standardize-features>Standardize features</a></li><li><a href=#suitability-tests>Suitability Tests</a><ul><li><a href=#bartletts-criterion>Bartlett&rsquo;s criterion</a></li><li><a href=#kaiser-meyer-olkin-criterion>Kaiser-Meyer-Olkin criterion</a></li></ul></li><li><a href=#create-covariance-matrix>Create Covariance Matrix</a></li><li><a href=#apply-matrix-decompensation-eigendecomposition>Apply Matrix decompensation (Eigendecomposition)</a></li><li><a href=#compute-and-evaluate-explained-variance>Compute and evaluate Explained Variance</a></li><li><a href=#projection-matrix>Projection Matrix</a></li><li><a href=#visualize-dataset>Visualize dataset</a></li></ul></nav></div></div><div class=content-margin><article><h1 id=introduction>Introduction</h1><p>As you might be well aware of Big Data isn&rsquo;t going anywhere anytime soon. Big Data Analytics, an active research area which is concerned with analyzing large scale, fast paced and varying types of data will stay a challenging field.</p><p>You might ask yourself - how this is linked to Principal Component Analysis? This statistical analysis is able to deal with one characteristic trait of Big Data. As you may recall <em>Big Data</em> is primarly characterized by <a href=https://www.ibmbigdatahub.com/infographic/four-vs-big-data>four Vs</a> (<em>Volume, Variety, Velocity, Veracity</em>).</p><p>If we take a look at the first trait we can think of <em>Volume</em> as the amount of sample points in a datasets as well as the <u>number of features</u> a dataset has to offer. If we have we work with a vast amount of variables we are talking about high dimensional data:</p><p>The <strong>following problems</strong> will most likely occur when you work with high dimensional data:</p><ul><li>Many <u>features are intercorrelated</u> with one another and many ML algorithmns can&rsquo;t work under this condition</li><li>It&rsquo;s <u>hard to identify</u> dependant variables with high predictive power</li><li><u>Visualizing data</u> which has more than three dimensions isn&rsquo;t possible</li></ul><p>As you maybe guessed already, a Principal Component Analysis is able to adress those problems.</p><br><h1 id=what-is-principal-component-analysis-pca>What is Principal Component Analysis (PCA)?</h1><p>But what is Principal Component Analysis? PCA is a <strong>dimensional reduction technique</strong> which aims to <u>reduce the number of variables</u> while maintaining as much information about them as possible. PCA is an method of Feature Extraction which means k variables get compressed to n variables (altough n &lt; k). Those newly created variables are called principal components, hence the name of the analysis. Ideally only a few <em>principal components</em> are able to explain the lion share of the variance in the original data.</p><p>As mentioned <strong>PCA</strong> extracts features and is thus <u>different from Feature Elimination</u> which is another type of dimensionality reduction. In contrast to Feature Elimination where dependant variables get elimated based on a certain criteria (e.g. when the correlation with the target variable is below a threshold value) the PCA retains all variables and only recomposes them. A great benefit is that all <strong>principal components are independent</strong> from each other and have zero intercorelation with one another. This makes modeling with linear type models a lot easier.</p><br><h1 id=when-can-pca-be-used>When can PCA be used?</h1><p>Ask yourself this <strong>three questions</strong> if you consider using PCA:</p><ul><li>Do you want to <u>reduce the number of variables</u>, but aren’t able to identify variables to completely remove from consideration?</li><li>Do you want to <u>ensure your variables are independent</u> of one another?</li><li>Are you comfortable making your independent variables <u>less interpretable</u>?</li></ul><p>If the answer to all questions is yes an PCA is probably a good idea. Before you apply PCA you should check if alle the <strong>formal requirements</strong> are met. Those requirements are:</p><ol><li>The <em>relationship between variables</em> should be linear</li><li>There shouldn&rsquo;t be any <em>outlier</em></li><li>The variables should be <em>continuus</em></li><li>The larger the sample size the more precise the result (n > 20 for each variable)</li></ol><br><h1 id=an-practial-example>An practial example</h1><h2 id=the-dataset>The Dataset</h2><p>We will be working with the <strong>Boston house prices</strong> dataset which is a data source which has been used in many Machine learning papers and is usually used for regression problems. But we won&rsquo;t build a prediction model here. We will be using the packages <code>pandas</code>, <code>numpy</code>, <code>matplotlib</code> and <code>sklearn</code> which are popular and user friendly modules for Data Science and Machine Learning.</p><pre class="mc-prism hide language-text"><code class=language-python># import packages
from sklearn.datasets import load_boston
from sklearn.preprocessing import StandardScaler
from factor_analyzer.factor_analyzer import calculate_kmo
from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D  # 3D Scatterplot

# load dataset
boston = load_boston()
df = pd.DataFrame(boston.data)
cols = boston.feature_names
df.columns = cols
df['MEDV'] = boston.target
</code></pre><pre class="mc-prism hide language-text"><code class=language-python>df.head(5)
</code></pre><div><style scoped>.dataframe tbody tr th:only-of-type{vertical-align:middle}<pre class="mc-prism hide language-text"><code>.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</code></pre><p></style></p><table border=1 class=dataframe><thead><tr style=text-align:right><th></th><th>CRIM</th><th>ZN</th><th>INDUS</th><th>CHAS</th><th>NOX</th><th>RM</th><th>AGE</th><th>DIS</th><th>RAD</th><th>TAX</th><th>PTRATIO</th><th>B</th><th>LSTAT</th><th>MEDV</th></tr></thead><tbody><tr><td>0</td><td>0.00632</td><td>18.0</td><td>2.31</td><td>0.0</td><td>0.538</td><td>6.575</td><td>65.2</td><td>4.0900</td><td>1.0</td><td>296.0</td><td>15.3</td><td>396.90</td><td>4.98</td><td>24.0</td></tr><tr><td>1</td><td>0.02731</td><td>0.0</td><td>7.07</td><td>0.0</td><td>0.469</td><td>6.421</td><td>78.9</td><td>4.9671</td><td>2.0</td><td>242.0</td><td>17.8</td><td>396.90</td><td>9.14</td><td>21.6</td></tr><tr><td>2</td><td>0.02729</td><td>0.0</td><td>7.07</td><td>0.0</td><td>0.469</td><td>7.185</td><td>61.1</td><td>4.9671</td><td>2.0</td><td>242.0</td><td>17.8</td><td>392.83</td><td>4.03</td><td>34.7</td></tr><tr><td>3</td><td>0.03237</td><td>0.0</td><td>2.18</td><td>0.0</td><td>0.458</td><td>6.998</td><td>45.8</td><td>6.0622</td><td>3.0</td><td>222.0</td><td>18.7</td><td>394.63</td><td>2.94</td><td>33.4</td></tr><tr><td>4</td><td>0.06905</td><td>0.0</td><td>2.18</td><td>0.0</td><td>0.458</td><td>7.147</td><td>54.2</td><td>6.0622</td><td>3.0</td><td>222.0</td><td>18.7</td><td>396.90</td><td>5.33</td><td>36.2</td></tr></tbody></table></div><h2 id=exploratory-data-analysis>Exploratory Data Analysis</h2><pre class="mc-prism hide language-text"><code class=language-python>df.info()
</code></pre><pre class="mc-prism hide language-text"><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 506 entries, 0 to 505
Data columns (total 14 columns):
CRIM       506 non-null float64
ZN         506 non-null float64
INDUS      506 non-null float64
CHAS       506 non-null float64
NOX        506 non-null float64
RM         506 non-null float64
AGE        506 non-null float64
DIS        506 non-null float64
RAD        506 non-null float64
TAX        506 non-null float64
PTRATIO    506 non-null float64
B          506 non-null float64
LSTAT      506 non-null float64
MEDV       506 non-null float64
dtypes: float64(14)
memory usage: 55.5 KB
</code></pre><p>As we can see clearly from the <code>df.info()</code> summary method we have 14 columns and 506 sample points. The <code>MEDV</code> column which is the median value of oner-occupied homes in $1000 steps is typically the target variable. All columns are formated as continuus variables. You could argue that a PCA doesn&rsquo;t make to much sense when you have only 13 dependant variables. For the sake of this example we will do it anyway.</p><p>Since we imported the dataset via an dedicated machine learning library called <code>sklearn</code> we have access to an detailed description method which gives us additional information about the dataset and its variables.</p><pre class="mc-prism hide language-text"><code class=language-python>description = boston.DESCR.split('\n')[0:29]  # only extract dataset characteristics
description = '\n'.join(description)
print(description)
</code></pre><pre class="mc-prism hide language-text"><code>.. _boston_dataset:

Boston house prices dataset
---------------------------

**Data Set Characteristics:**  

    :Number of Instances: 506 

    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.

    :Attribute Information (in order):
        - CRIM     per capita crime rate by town
        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.
        - INDUS    proportion of non-retail business acres per town
        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
        - NOX      nitric oxides concentration (parts per 10 million)
        - RM       average number of rooms per dwelling
        - AGE      proportion of owner-occupied units built prior to 1940
        - DIS      weighted distances to five Boston employment centres
        - RAD      index of accessibility to radial highways
        - TAX      full-value property-tax rate per $10,000
        - PTRATIO  pupil-teacher ratio by town
        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
        - LSTAT    % lower status of the population
        - MEDV     Median value of owner-occupied homes in $1000's

    :Missing Attribute Values: None
</code></pre><p>There are <em>several categories</em> of variables in this dataset. We can see <u>socioeconomic, educational, environmental and property related variables</u>. The hint that there are no missing values is helpful since we usually had to check this for ourselves. In general the absence of missing values makes working with data a lot easier.</p><p>After doing a quick exploration of the data, let&rsquo;s finally apply the PCA. Here are the <strong>steps necessary to apply the Principal Component Analysis</strong> to this dataset. The general layout of those steps are the following:</p><ol><li>Seperate dependant and independant* variables</li><li>Standardize the features</li><li>Suitability Tests</li><li>Calculate Covariance Matrix</li><li>Apply Matrix decompensation (Eigendecomposition)</li><li>Compute and evaluate Explained Variance</li><li>Construct Projection Matrix</li><li>Visualize dataset</li></ol><h2 id=seperate-dependant-and-independant-variables>Seperate dependant and independant variables</h2><p>We start by seperating predictor and target variables since the PCA should be <u>applied to the dependant variables only.</u></p><pre class="mc-prism hide language-text"><code class=language-python>X, y = df.iloc[:, 0:13], df.iloc[:, 13]
X.head(5)  # dependant variables
</code></pre><div><style scoped>.dataframe tbody tr th:only-of-type{vertical-align:middle}<pre class="mc-prism hide language-text"><code>.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</code></pre><p></style></p><table border=1 class=dataframe><thead><tr style=text-align:right><th></th><th>CRIM</th><th>ZN</th><th>INDUS</th><th>CHAS</th><th>NOX</th><th>RM</th><th>AGE</th><th>DIS</th><th>RAD</th><th>TAX</th><th>PTRATIO</th><th>B</th><th>LSTAT</th></tr></thead><tbody><tr><td>0</td><td>0.00632</td><td>18.0</td><td>2.31</td><td>0.0</td><td>0.538</td><td>6.575</td><td>65.2</td><td>4.0900</td><td>1.0</td><td>296.0</td><td>15.3</td><td>396.90</td><td>4.98</td></tr><tr><td>1</td><td>0.02731</td><td>0.0</td><td>7.07</td><td>0.0</td><td>0.469</td><td>6.421</td><td>78.9</td><td>4.9671</td><td>2.0</td><td>242.0</td><td>17.8</td><td>396.90</td><td>9.14</td></tr><tr><td>2</td><td>0.02729</td><td>0.0</td><td>7.07</td><td>0.0</td><td>0.469</td><td>7.185</td><td>61.1</td><td>4.9671</td><td>2.0</td><td>242.0</td><td>17.8</td><td>392.83</td><td>4.03</td></tr><tr><td>3</td><td>0.03237</td><td>0.0</td><td>2.18</td><td>0.0</td><td>0.458</td><td>6.998</td><td>45.8</td><td>6.0622</td><td>3.0</td><td>222.0</td><td>18.7</td><td>394.63</td><td>2.94</td></tr><tr><td>4</td><td>0.06905</td><td>0.0</td><td>2.18</td><td>0.0</td><td>0.458</td><td>7.147</td><td>54.2</td><td>6.0622</td><td>3.0</td><td>222.0</td><td>18.7</td><td>396.90</td><td>5.33</td></tr></tbody></table></div><h2 id=standardize-features>Standardize features</h2><pre class="mc-prism hide language-text"><code class=language-python>import warnings
warnings.simplefilter(&quot;ignore&quot;, UserWarning)
fig = plt.figure(figsize = (20,12))
ax = fig.gca()
df.hist(ax = ax);
</code></pre><p><img src=/assets/2019-12-30-introduction-to-principal-component-analysis/output_31_0.png alt=png></p><p>As we can see above the scale varies quite a lot between the different variables. This makes a Rescaling necessary for the PCA to work properly. We are using the the <code>StandardScaler</code> from the <code>sklearn.preprocessing</code> module to transform the data onto unit scale (mean = 0, variance = 1).</p><pre class="mc-prism hide language-text"><code class=language-python>X_std = StandardScaler().fit_transform(X)
</code></pre><h2 id=suitability-tests>Suitability Tests</h2><p>Before we can apply the necessary steps of a PCA we have to perform two statistical tests which measure if a PCA is suitable for the present dataset, the <code>Bartlett</code> and <code>Kaiser-Meyer-Olkin</code> test.</p><h3 id=bartletts-criterion>Bartlett&rsquo;s criterion</h3><p>Bartlett’s test of sphericity checks the null hypothesis that the correlation matrix is equal to the identity matrix. The <strong>identiy matrix</strong> is a n × n square unit matrix with ones on the main diagonal and zeros elsewhere. The <strong>correlation matrix</strong> is a table showing correlation coefficients between variables. The correlation coefficient measure the linear relationship between two variables. The <u>hypothesis should be rejected</u>, so we can continue with the PCA.</p><pre class="mc-prism hide language-text"><code class=language-python># Test Hypothesis
chi_square_value,p_value=calculate_bartlett_sphericity(df)
print('Chi Square Value: \n%s' %chi_square_value)
print('p-Value: \n%s' %p_value)
</code></pre><p>The p-value of the Bartlett test is zero and therefore statistically significant. We can conclude that the <em>correlation matrix</em> is <u>not equal to</u> the <em>identity matrix</em> and proceed with the next test.</p><h3 id=kaiser-meyer-olkin-criterion>Kaiser-Meyer-Olkin criterion</h3><p>The <em>Kaiser-Meyer-Olkin</em> (KMO) test also measures the suitability of a dataset for Principal component analysis. The minimal overall value for all features <u>should be above 0.6</u>.</p><pre class="mc-prism hide language-text"><code class=language-python># Calculate KMO
kmo_all, kmo_model = calculate_kmo(X_std)
print('Overall KMO: \n%s' %kmo_model)
</code></pre><p>The <em>overall value</em> is <strong>0.85</strong> which is a fantastic value. This value indicates that we can proceed with the next steps.</p><h2 id=create-covariance-matrix>Create Covariance Matrix</h2><p>The <em>covariance matrix</em> is a measure which measures <u>how variables are associated with one another</u>. The covariance between two variables can be computed by this formular:</p><p>$$ \sigma_{j k}=\frac{1}{n-1} \sum_{i=1}^{n}\left(x_{i j}-\bar{x}<em>{j}\right)\left(x</em>{i k}-\bar{x}_{k}\right) $$</p><p>We calculate the Covariance Matrix because the <em>Eigendecomposition</em> which is applied in the classic method of a PCA requires this matrix.</p><pre class="mc-prism hide language-text"><code class=language-python>cov_mat = np.cov(X_std.T)
print('Covariance matrix: \n%s' %cov_mat[0:4])  # only a sample of features
</code></pre><h2 id=apply-matrix-decompensation-eigendecomposition>Apply Matrix decompensation (Eigendecomposition)</h2><p><strong>Eigendecomposition</strong> of a matrix is a type of decomposition that involves <u>decomposing a square matrix</u> into a set of <em>eigenvectors</em> and <em>eigenvalues</em>. We use Singular Value Decomposition (SVD) which is the fastest approach to do a Matrix decomposition.</p><p><strong>Eigenvectors</strong> are <em>unit vectors</em>, which means that their length or magnitude is equal to 1.0. Eigenvectors are <em>column vectors</em> with shape (1, n). <strong>Eigenvalues</strong> are coefficients applied to eigenvectors that give the vectors their length or magnitude.</p><pre class="mc-prism hide language-text"><code class=language-python>eig_vecs, eig_vals, _ = np.linalg.svd(cov_mat)
</code></pre><pre class="mc-prism hide language-text"><code class=language-python>print('Eigenvectors \n%s' %eig_vecs[0:4])
print('\nEigenvalues \n%s' %eig_vals)
</code></pre><pre class="mc-prism hide language-text"><code>Eigenvectors 
[[-0.2509514  -0.31525237  0.24656649  0.06177071  0.08215692 -0.21965961
   0.77760721 -0.15335048  0.26039028 -0.01936913  0.10964435  0.08676107
  -0.0459523 ]
 [ 0.25631454 -0.3233129   0.29585782  0.12871159  0.32061699 -0.3233881
  -0.27499628  0.40268031  0.35813749 -0.26752723 -0.26275629 -0.07142528
   0.08091897]
 [-0.34667207  0.11249291 -0.01594592  0.01714571 -0.00781119 -0.0761379
  -0.33957645 -0.17393172  0.64441615  0.36353226  0.30316943 -0.11319963
   0.25107654]
 [-0.00504243  0.45482914  0.28978082  0.81594136  0.08653094  0.16749014
   0.07413621  0.02466215 -0.01372777  0.00618184 -0.01392667 -0.00398268
  -0.03592171]]

Eigenvalues 
[6.1389812  1.43611329 1.2450773  0.85927328 0.83646904 0.65870897
 0.5364162  0.39688167 0.27749173 0.22067394 0.18638271 0.16963823
 0.06363502]
</code></pre><h2 id=compute-and-evaluate-explained-variance>Compute and evaluate Explained Variance</h2><p>The <em>eigenvectors</em> only define the direction or magnitude of an axis since their unit length is normed to a length of 1. In order to decide <u>which eigenvectors should be dropped</u> when creating a lower dimensional subspace we have to evaluate the <em>eigenvalues</em>. The <strong>explained variance</strong> is a measurement based on these eigenvalues which can tell us how much information can be attributed to each principal component.</p><pre class="mc-prism hide language-text"><code class=language-python># Calculate individual and cumulated variance
tot_var = sum(eig_vals)
exp_var = (eig_vals / tot_var)*100
cum_exp_var = np.cumsum(exp_var)
</code></pre><pre class="mc-prism hide language-text"><code class=language-python># Plot variance to graph
plt.figure(figsize=(8, 6))
plt.bar(range(len(exp_var)), exp_var, 
        label='individual explained variance')
plt.step(range(len(exp_var)), cum_exp_var, where='mid',
         label='cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal components')
plt.legend(loc='center')
plt.tight_layout();
</code></pre><p><img src=/assets/2019-12-30-introduction-to-principal-component-analysis/output_61_0.png alt=png></p><p>The essential question now is: <u>How do we choose the number of principal components</u> which should represent our dataset? There are basically <strong>four viable methods</strong> for picking the number of components:</p><ul><li><strong>Method 1</strong>: We select how many dimensions we want to keep based on our analysis goal. Perhaps we want to visually represent data in two dimensions which would mean we keep two principal components</li><li><strong>Method 2</strong>: Calculate the cumulative explained variance (as in the graph above) and select as many principal components until a defined threshold is reached (typical would be a value of 80%).</li><li><strong>Method 3</strong>: Calculate the individual explained variance for each component and look for a significant drop in explained variance. Then pick alle components before the big drop. This approach is also called &lsquo;find the elbow&rsquo;.</li><li><strong>Method 4</strong>: Compute the individual explained variance and only pick those princripal components which explain at least 10% of the total variance</li></ul><h2 id=projection-matrix>Projection Matrix</h2><p>Finally we will construct the <strong>Projection Matrix W</strong> which we will use to transform the orginal dataset to a lower dimensional space. In this our case i will choose the three principal components as basis for the projection matrix since <u>i want to create a three dimensional visualization</u> of our dataset (<em>Method 1</em>).</p><pre class="mc-prism hide language-text"><code class=language-python># Pick first three eigenvectors
matrix_w = eig_vecs[:, 0:3]
print(f'\nShape of Projection Matrix: {matrix_w.shape}')
print('\nProjection Matrix: \n%s' %matrix_w)
</code></pre><p>With help of this <strong>Projection Matrix</strong> we can <u>create the lower dimensional dataset</u> which contains new values for each observation. As you can see the new dataset only has three dependant variables: <code>PC1</code>, <code>PC2</code>, <code>PC3</code>. I created three new values (<code>cheap</code>, <code>moderate</code>, <code>expensive</code>) for the target variable <code>MEDV</code> which represents the price of a house and will be used for coloring.</p><pre class="mc-prism hide language-text"><code class=language-python># Create new feature space with principal components
result = pd.DataFrame(X_std.dot(matrix_w))
result.columns = ['PC1', 'PC2', 'PC3']
result = pd.concat([result, pd.cut(y, bins=[0, 15, 30, 50], 
                    labels=['cheap', 'moderate', 'expensive'])], axis=1)
</code></pre><pre class="mc-prism hide language-text"><code class=language-python>result.head(10)
</code></pre><div><style scoped>.dataframe tbody tr th:only-of-type{vertical-align:middle}<pre class="mc-prism hide language-text"><code>.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</code></pre><p></style></p><table border=1 class=dataframe><thead><tr style=text-align:right><th></th><th>PC1</th><th>PC2</th><th>PC3</th><th>MEDV</th></tr></thead><tbody><tr><td>0</td><td>2.098297</td><td>0.773113</td><td>0.342943</td><td>moderate</td></tr><tr><td>1</td><td>1.457252</td><td>0.591985</td><td>-0.695199</td><td>moderate</td></tr><tr><td>2</td><td>2.074598</td><td>0.599639</td><td>0.167122</td><td>expensive</td></tr><tr><td>3</td><td>2.611504</td><td>-0.006871</td><td>-0.100284</td><td>expensive</td></tr><tr><td>4</td><td>2.458185</td><td>0.097712</td><td>-0.075348</td><td>expensive</td></tr><tr><td>5</td><td>2.214852</td><td>-0.009487</td><td>-0.672381</td><td>moderate</td></tr><tr><td>6</td><td>1.358881</td><td>0.349872</td><td>-0.371999</td><td>moderate</td></tr><tr><td>7</td><td>0.842045</td><td>0.577800</td><td>-0.518540</td><td>moderate</td></tr><tr><td>8</td><td>0.179928</td><td>0.342518</td><td>-1.349639</td><td>moderate</td></tr><tr><td>9</td><td>1.074184</td><td>0.316201</td><td>-0.558469</td><td>moderate</td></tr></tbody></table></div><h2 id=visualize-dataset>Visualize dataset</h2><p>Finally let&rsquo;s plot the <em>principal components</em> in a <u>three dimensional space</u> and use the recoded target variable for colouring.</p><pre class="mc-prism hide language-text"><code class=language-python># Plot initialisation
fig = plt.figure(figsize=(12,8))
ax = fig.add_subplot(111, projection='3d')
ax.scatter(result['PC1'], result['PC2'], result['PC3'], c=result['MEDV'].cat.codes, cmap=&quot;Set2_r&quot;, s=60)
 
# Make simple, bare axis lines through space (red):
xAxisLine = ((min(result['PC1']), max(result['PC1'])), (0, 0), (0,0))
ax.plot(xAxisLine[0], xAxisLine[1], xAxisLine[2], 'r')
yAxisLine = ((0, 0), (min(result['PC2']), max(result['PC2'])), (0,0))
ax.plot(yAxisLine[0], yAxisLine[1], yAxisLine[2], 'r')
zAxisLine = ((0, 0), (0,0), (min(result['PC3']), max(result['PC3'])))
ax.plot(zAxisLine[0], zAxisLine[1], zAxisLine[2], 'r')
 
# Label the axes, title and legend
ax.set_xlabel(&quot;PC1&quot;)
ax.set_ylabel(&quot;PC2&quot;)
ax.set_zlabel(&quot;PC3&quot;)
ax.set_title(&quot;Boston Housing Prices - PCA results&quot;);
</code></pre><p><img src=/assets/2019-12-30-introduction-to-principal-component-analysis/output_73_0.png alt=png></p><h1 id=summary>Summary</h1><hr><p>Let&rsquo;s summarize what what have learned about PCA:</p><ul><li>PCA is used to <u>overcome features redundancy</u> in a data set.</li><li>These features are low dimensional in nature.</li><li>These features a.k.a components are a resultant of normalized <em>linear combination</em> of original predictor variables.</li><li>These components aim to capture as much information as possible with <em>high explained variance</em>.</li><li>The <em>first component</em> has the <u>highest variance</u> followed by second, third and so on.</li><li><em>Normalizing data</em> becomes extremely <u>important</u> when the predictors are measured in different units.</li><li>PCA works best on data set having 3 or higher dimensions. Because, with higher dimensions, it becomes increasingly difficult to make interpretations from the - resultant cloud of data.</li><li>PCA is applied on a data set with <u>numeric variables</u>.</li><li>PCA is a tool which helps to produce <u>better visualizations of high dimensional data</u>.</li></ul></article></div></div><footer><article>Copyright © 2023 by Marc-Philipp Esser</article></footer></main></body></html>