<!doctype html><html lang=en><head><meta charset=utf-8><link rel="shortcut icon" href=/favicon.ico type=image/x-icon><meta name=viewport content="width=device-width,initial-scale=1"><title>Dealing with categorical data - Marc-Philipp Esser | Personal Website</title><meta name=description content="Dealing with categorical data"><meta name=keywords content="encoding,feature-engineering,data-science,python"><meta name=author content="Marc-Philipp Esser"><meta property="og:title" content="Dealing with categorical data"><meta property="og:description" content="Dealing with categorical data"><meta property="og:type" content="article"><meta property="og:url" content="https://example.com/blog/dealing-with-categorical-data/"><meta property="article:section" content="blog"><meta property="article:published_time" content="2019-12-29T00:00:00+00:00"><meta property="article:modified_time" content="2019-12-29T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Dealing with categorical data"><meta name=twitter:description content="Dealing with categorical data"><link rel=stylesheet href=/style.min.5297c96c59a52afaa5bcda4a6cedf3813081f64025c209b25b2ee6d0c8f74d462b625ad3404a92a14d7a51b4ec0a420337ae70f426fa4bce2d5f7459a3ca7274.css integrity="sha512-UpfJbFmlKvqlvNpKbO3zgTCB9kAlwgmyWy7m0Mj3TUYrYlrTQEqSoU16UbTsCkIDN65w9Cb6S84tX3RZo8pydA=="><link rel=stylesheet href=/lib/css/prism.min.6226f06f992e0d6166b0e26724efd050dcc381202a752892ba523b1b865de2ea5e427f8f7d10de682fc35d6e7444018247d1f25db5e1e3bab17068ce191c5886.css integrity="sha512-Yibwb5kuDWFmsOJnJO/QUNzDgSAqdSiSulI7G4Zd4upeQn+PfRDeaC/DXW50RAGCR9HyXbXh47qxcGjOGRxYhg=="><script>localStorage.theme==="dark"||!("theme"in localStorage)&&window.matchMedia("(prefers-color-scheme: dark)").matches?document.documentElement.setAttribute("data-theme","dark"):document.documentElement.setAttribute("data-theme","light")</script><script defer src=/js/header.7a2a109ec3782c57bad0332b662f8a5f41765505936b69868eb8bd5241de9daf23c388e82ca1831f6d09935013dcb9f71bfa7face3975880c1076028b7b0a6e1.js integrity="sha512-eioQnsN4LFe60DMrZi+KX0F2VQWTa2mGjri9UkHena8jw4joLKGDH20Jk1AT3Ln3G/p/rOOXWIDBB2Aot7Cm4Q=="></script>
<script defer src=/js/zooming.684b5d075bf94d0adfa21a7e7eb9acec1ddfb2e7b47d6657981617f0db0cf50949f1172801595afa3051f51b28d67f6a2d0c41be677b59b564307d9dbe4a4fd2.js integrity="sha512-aEtdB1v5TQrfohp+frms7B3fsue0fWZXmBYX8NsM9QlJ8RcoAVla+jBR9Rso1n9qLQxBvmd7WbVkMH2dvkpP0g=="></script>
<script defer src=/js/prism.c16efe6b4f0ae7df5f934ad76ec45633d61b926e66c11dae8f2f56f5e6fc27ba412a3f88ba617f8436480ac928ca3fbc0c3cdee84992a64130d42ea551dad860.js integrity="sha512-wW7+a08K599fk0rXbsRWM9Ybkm5mwR2ujy9W9eb8J7pBKj+IumF/hDZICskoyj+8DDze6EmSpkEw1C6lUdrYYA==" data-manual></script>
<script defer src=/js/search-en.677e05c714f837051483647d736c45051126c22a2696ca5f981b59da962d48962f3ea71150f559ba84884fd1aadf1c7aadce55bb31a3e5b75c0f173b763c109a.js integrity="sha512-Z34FxxT4NwUUg2R9c2xFBREmwiomlspfmBtZ2pYtSJYvPqcRUPVZuoSIT9Gq3xx6rc5VuzGj5bdcDxc7djwQmg=="></script></head><body><main><header><div class=brand><div id=sidebar_btn><svg id="menu_icon" width="26" height="26" viewBox="0 0 24 24"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2.5" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></svg></div><div><a href=/>Marc-Philipp Esser</a></div></div><div class=toolbox><div id=theme_tool><svg id="dark_mode_btn" class="hidden toolbox-btn" width="18" height="18" viewBox="0 0 24 24"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2.5" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></svg><svg id="light_mode_btn" class="hidden toolbox-btn" width="18" height="18" viewBox="0 0 24 24"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg></svg></div><div id=search_tool><svg id="search_btn" class="toolbox-btn" width="18" height="18" viewBox="0 0 24 24"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2.5" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"><circle cx="11" cy="11" r="8"/><line x1="21" y1="21" x2="16.65" y2="16.65"/></svg></svg><div id=search_menu_wrapper class=hidden><div id=search_menu><div id=search_menu_toolbar><div id=search_menu_input_wrapper><input id=search_menu_input type=text placeholder='Search Posts'></div><div id=search_menu_close_btn><svg width="18" height="18" viewBox="0 0 24 24"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-x"><line x1="18" y1="6" x2="6" y2="18"/><line x1="6" y1="6" x2="18" y2="18"/></svg></svg></div></div><div id=search_menu_results></div></div></div></div></div></header><nav id=navbar class=pure-menu><ul class=pure-menu-list><li class="navbar-item pure-menu-item"><a href=/cv/ class=pure-menu-link>Curricum Vitae</a></li><li class="navbar-item pure-menu-item insection"><a href=/blog/ class=pure-menu-link>Blog</a></li><li class="navbar-item pure-menu-item"><a href=/project/ class=pure-menu-link>Projects</a></li></ul></nav><div id=sidebar_canvas_overlay class=hidden></div><div id=sidebar class=close><ul><li><a href=/cv/>Curricum Vitae</a></li><li><a href=/blog/>Blog</a></li><li><a href=/project/>Projects</a></li></ul></div><div id=content class=content-margin><div class=collapsible-menu-wrapper><div class=collapsible-menu-type><span>Table of contents</span></div><div class=collapsible-menu><nav id=TableOfContents><ul><li><a href=#one-hot-encoding>One Hot Encoding</a></li><li><a href=#ordinal-encoding>Ordinal Encoding</a></li><li><a href=#binary-encoding>Binary Encoding</a></li><li><a href=#frequency-encoding>Frequency Encoding</a></li><li><a href=#hashing-encoding>Hashing Encoding</a></li><li><a href=#sum-encoding>Sum Encoding</a></li><li><a href=#target-encoding>Target Encoding</a></li><li><a href=#leave-one-out-encoding>Leave One Out Encoding</a></li></ul></nav></div></div><div class=content-margin><article><h1 id=introduction>Introduction</h1><p>In real-world datasets it is often the case that you have a mixed variable types. While <strong>Machine learning algorithmns</strong> usually can handle only numerical data, they <strong>can&rsquo;t work with categorical variables</strong>. Being able to use categorical variables in modeling makes it necessary to transform those variables. This process is called <strong>Encoding</strong> and there are many different strategies for it.</p><p>There are two types of variables which can be seen as categorical variables:</p><ol><li><p><strong>Nominal variables</strong>: Nominal data is made of discrete values with no numerical relationship between the different values — mean and median are meaningless. An example here would be the colour of a car or the job title of a person.</p></li><li><p><strong>Ordinal variables</strong>: A variable used to rank a sample of individuals with respect to some characteristics, but differences (i.e., intervals) and different points of the scale are not necessarily equivalent. An example here would be the</p></li></ol><br><h1 id=basic-encoding-strategies>Basic Encoding Strategies</h1><p>The following <em>encoding strategies</em> are easy to understand and very popular in Machine Learning:</p><ul><li>One Hot Encoding</li><li>Ordinal Encoding</li><li>Binary Encoding</li><li>Frequency Encoding</li><li>Hashing Encoding</li><li>Sum Encoding</li><li>Mean Encoding</li><li>Leave One Out Encoding</li></ul><p>To illustrate these different encoding strategies i will be using an <em>sample dataset</em> which i manuelly created in Excel. You can find it in this <a href=https://github.com/m-p-esser/dealing-with-categorical-data>Github Repository</a>.</p><p>Okay let&rsquo;s begin by importing all necessary python packages. If your working with the <a href=https://www.anaconda.com/distribution/>Anaconda Distribution</a> all packages except for the <code>category_encoders</code> module should be preinstalled. Installation instructions for this package can be found <a href=https://contrib.scikit-learn.org/categorical-encoding/>here</a>.</p><pre class="mc-prism hide language-text"><code class=language-python># import packages
import os
import numpy as np
import pandas as pd
import category_encoders as ce

# load dataset
root_dir = os.path.abspath(os.pardir)
data_dir = os.path.join(root_dir, 'data')
df = pd.read_excel(os.path.join(data_dir, 'dummy_dataset.xlsx'))

# seperate predictor and target variable
X = df.loc[:, ['id', 'age', 'iq', 'hair']]
y = df.loc[:, ['target']]
</code></pre><p>Let&rsquo;s have a look at the dataset:</p><pre class="mc-prism hide language-text"><code class=language-python>df.head(10)
</code></pre><div><style scoped>.dataframe tbody tr th:only-of-type{vertical-align:middle}<pre class="mc-prism hide language-text"><code>.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</code></pre><p></style></p><table border=1 class=dataframe><thead><tr style=text-align:right><th></th><th>id</th><th>age</th><th>iq</th><th>hair</th><th>target</th></tr></thead><tbody><tr><td>0</td><td>1</td><td>0-17</td><td>very low</td><td>brown</td><td>0</td></tr><tr><td>1</td><td>2</td><td>18-29</td><td>low</td><td>black</td><td>1</td></tr><tr><td>2</td><td>3</td><td>30-49</td><td>medium</td><td>blonde</td><td>0</td></tr><tr><td>3</td><td>4</td><td>50-69</td><td>high</td><td>black</td><td>1</td></tr><tr><td>4</td><td>5</td><td>70+</td><td>very high</td><td>blonde</td><td>0</td></tr><tr><td>5</td><td>6</td><td>18-29</td><td>medium</td><td>brown</td><td>1</td></tr><tr><td>6</td><td>7</td><td>30-49</td><td>high</td><td>blonde</td><td>1</td></tr><tr><td>7</td><td>8</td><td>30-49</td><td>low</td><td>brown</td><td>1</td></tr><tr><td>8</td><td>9</td><td>50-69</td><td>medium</td><td>brown</td><td>0</td></tr><tr><td>9</td><td>10</td><td>18-29</td><td>high</td><td>black</td><td>0</td></tr></tbody></table></div><br><h2 id=one-hot-encoding>One Hot Encoding</h2><p>I&rsquo;ll mention this technique first since it is the go-to approach to encode data and very easy to understand. This approach is also called <em>dummy or indicator encoding</em>. When transforming a variable via One Hot Encoding <u>every unique value of the original columns get it&rsquo;s own column</u>. This means the number of columns in a dataframe gets increased by k-1 (where k is the number of unique values). This type of encoding can be applied to <em>nominal</em> as well as <em>categorical variables</em>.</p><pre class="mc-prism hide language-text"><code class=language-python>one_hot_enc = ce.OneHotEncoder(cols=['hair'], use_cat_names=True)
encoding = one_hot_enc.fit_transform(X['hair'], y)
combined = pd.concat([X['hair'], encoding], axis=1)
combined.sort_values('hair')
</code></pre><div><style scoped>.dataframe tbody tr th:only-of-type{vertical-align:middle}<pre class="mc-prism hide language-text"><code>.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</code></pre><p></style></p><table border=1 class=dataframe><thead><tr style=text-align:right><th></th><th>hair</th><th>hair_brown</th><th>hair_black</th><th>hair_blonde</th></tr></thead><tbody><tr><td>1</td><td>black</td><td>0</td><td>1</td><td>0</td></tr><tr><td>3</td><td>black</td><td>0</td><td>1</td><td>0</td></tr><tr><td>9</td><td>black</td><td>0</td><td>1</td><td>0</td></tr><tr><td>2</td><td>blonde</td><td>0</td><td>0</td><td>1</td></tr><tr><td>4</td><td>blonde</td><td>0</td><td>0</td><td>1</td></tr><tr><td>6</td><td>blonde</td><td>0</td><td>0</td><td>1</td></tr><tr><td>0</td><td>brown</td><td>1</td><td>0</td><td>0</td></tr><tr><td>5</td><td>brown</td><td>1</td><td>0</td><td>0</td></tr><tr><td>7</td><td>brown</td><td>1</td><td>0</td><td>0</td></tr><tr><td>8</td><td>brown</td><td>1</td><td>0</td><td>0</td></tr></tbody></table></div><p>As you can see each hair color gets it&rsquo;s own column. Since a person can only have one hair color there is only a <u>single 1 in each row</u>. As you might also imagine the <u>number of columns created by this approach can very large</u> and therefore computionally and memory expensive especially if we deal with <strong>high cardinality features</strong>. High cardinality features are variables which have a lot of unique values. An good example here are zip codes.</p><br><h2 id=ordinal-encoding>Ordinal Encoding</h2><p>This technique as the name suggests can <u>only be applied to ordinal features</u>. Here each string will be replaced by an corresponding integer. This replacement make sense and is recommended since there is an <em>natural order</em> in ordinal variables. Furthermore this approach is <strong>very cost effective</strong> because no additional columns are created.</p><pre class="mc-prism hide language-text"><code class=language-python>ordinal_enc = ce.OrdinalEncoder(cols=['iq'], 
                                mapping=[{'col':'iq', 'mapping':
                                          {'very low': 1, 'low': 2, 'medium': 3, 
                                           'high': 4, 'very high': 5}}])
encoding = ordinal_enc.fit_transform(X['iq'], y).rename(columns={'iq':'iq_enc'})
combined = pd.concat([X['iq'], encoding], axis=1)
combined.sort_values('iq_enc')
</code></pre><div><style scoped>.dataframe tbody tr th:only-of-type{vertical-align:middle}<pre class="mc-prism hide language-text"><code>.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</code></pre><p></style></p><table border=1 class=dataframe><thead><tr style=text-align:right><th></th><th>iq</th><th>iq_enc</th></tr></thead><tbody><tr><td>0</td><td>very low</td><td>1</td></tr><tr><td>1</td><td>low</td><td>2</td></tr><tr><td>7</td><td>low</td><td>2</td></tr><tr><td>2</td><td>medium</td><td>3</td></tr><tr><td>5</td><td>medium</td><td>3</td></tr><tr><td>8</td><td>medium</td><td>3</td></tr><tr><td>3</td><td>high</td><td>4</td></tr><tr><td>6</td><td>high</td><td>4</td></tr><tr><td>9</td><td>high</td><td>4</td></tr><tr><td>4</td><td>very high</td><td>5</td></tr></tbody></table></div><p>The following things happend when applying the encoding:</p><ul><li>The value <strong>&lsquo;very low&rsquo;</strong> got replaced by a <strong>1</strong>, the value <strong>low</strong> got replaced by a <strong>2</strong> and so fourth</li><li>The <u>natural order has been preserved</u> because of the mapping argument which the encoder has been given. Otherwise the assignment would have been random.</li></ul><br><h2 id=binary-encoding>Binary Encoding</h2><p>This encoding method can be seen as <u>hybrid between One Hot and Hashing Encoders</u>. It creates fewer features as the One Hot Encoding approach while preserving a more unique character of values. It works very well with <em>high dimensional ordinal data</em> altough this combination is very rare.</p><pre class="mc-prism hide language-text"><code class=language-python>binary_enc = ce.BinaryEncoder(cols=['hair'])
encoding = binary_enc.fit_transform(X['hair'], y)
combined = pd.concat([X['hair'], encoding], axis=1)
combined.sort_values('hair')
</code></pre><div><style scoped>.dataframe tbody tr th:only-of-type{vertical-align:middle}<pre class="mc-prism hide language-text"><code>.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</code></pre><p></style></p><table border=1 class=dataframe><thead><tr style=text-align:right><th></th><th>hair</th><th>hair_0</th><th>hair_1</th><th>hair_2</th></tr></thead><tbody><tr><td>1</td><td>black</td><td>0</td><td>1</td><td>0</td></tr><tr><td>3</td><td>black</td><td>0</td><td>1</td><td>0</td></tr><tr><td>9</td><td>black</td><td>0</td><td>1</td><td>0</td></tr><tr><td>2</td><td>blonde</td><td>0</td><td>1</td><td>1</td></tr><tr><td>4</td><td>blonde</td><td>0</td><td>1</td><td>1</td></tr><tr><td>6</td><td>blonde</td><td>0</td><td>1</td><td>1</td></tr><tr><td>0</td><td>brown</td><td>0</td><td>0</td><td>1</td></tr><tr><td>5</td><td>brown</td><td>0</td><td>0</td><td>1</td></tr><tr><td>7</td><td>brown</td><td>0</td><td>0</td><td>1</td></tr><tr><td>8</td><td>brown</td><td>0</td><td>0</td><td>1</td></tr></tbody></table></div><p>Altough it first might seem like this technique is exactly like One Hot Encoding, the appearances are deceptive. If you observe the third row (index = 2) you will notice that there are two &lsquo;1s&rsquo; in the whole row. This wouldn&rsquo;t be possible in a One Hot Encoding. This row tells us that hair color of the respondent is <code>blond</code>. Blond is the third type of hair colour and three is represented by 011 in the <strong>binary language</strong>.</p><p><strong>Binary Encoding follow these steps</strong>:</p><ul><li>The categories are encoded by an Ordinal Encoder if they aren’t already in numeric form</li><li>Then those integers are converted into binary code, so for example 5 becomes 101 and 8 becomes 112.</li><li>Then the digits from that binary string are split into separate columns. So if there are 4–7 values in an ordinal column then 3 new columns are created: one for the first bit, one for the second, and one for the third.</li><li>Each observation is encoded across the columns in its binary form.</li></ul><br><h2 id=frequency-encoding>Frequency Encoding</h2><p>This encoding approach is <strong>rather uncommon</strong> and can be used for <em>nominal</em> as well as <em>ordinal</em> features. In this case the values get replaced by their <u>frequency in relation to the whole dataset</u>, hence the name. In this case we won&rsquo;t use the <code>category_encoders</code> package. Instead we&rsquo;ll use panda methods to encode the data.</p><pre class="mc-prism hide language-text"><code class=language-python>freq_enc = df.groupby('hair').size() / len(df)
df.loc[:, 'hair_enc'] = df['hair'].map(freq_enc)
df.loc[:, ['hair', 'hair_enc']].sort_values('hair').sort_values('hair_enc')
</code></pre><div><style scoped>.dataframe tbody tr th:only-of-type{vertical-align:middle}<pre class="mc-prism hide language-text"><code>.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</code></pre><p></style></p><table border=1 class=dataframe><thead><tr style=text-align:right><th></th><th>hair</th><th>hair_enc</th></tr></thead><tbody><tr><td>1</td><td>black</td><td>0.3</td></tr><tr><td>3</td><td>black</td><td>0.3</td></tr><tr><td>9</td><td>black</td><td>0.3</td></tr><tr><td>2</td><td>blonde</td><td>0.3</td></tr><tr><td>4</td><td>blonde</td><td>0.3</td></tr><tr><td>6</td><td>blonde</td><td>0.3</td></tr><tr><td>0</td><td>brown</td><td>0.4</td></tr><tr><td>5</td><td>brown</td><td>0.4</td></tr><tr><td>7</td><td>brown</td><td>0.4</td></tr><tr><td>8</td><td>brown</td><td>0.4</td></tr></tbody></table></div><p>Since the color brown is the most frequent value in the <code>hair</code> column is has the highest value in the encoded columns followed by the other two values. Instead of relative value <u>we could also use absolute frequencies</u> (counts).</p><br><h2 id=hashing-encoding>Hashing Encoding</h2><p>This technique implements the <strong>hashing trick</strong>. It is <em>similar to one-hot encoding</em> but with less newly created columns and some loss in information because of <em>collision effects</em>. The collisions do not significantly affect performance unless there is a great deal of overlap. An detailed discussion of this method can be found <a href=https://booking.ai/dont-be-tricked-by-the-hashing-trick-192a6aae3087>here</a> and an in-depth explanation can be found in <a href=https://medium.com/value-stream-design/introducing-one-of-the-best-hacks-in-machine-learning-the-hashing-trick-bf6a9c8af18f>this article</a>.</p><pre class="mc-prism hide language-text"><code class=language-python>hash_enc = ce.HashingEncoder(cols=['hair'])
encoding = hash_enc.fit_transform(X['hair'], y)
combined = pd.concat([X['hair'], encoding], axis=1)
combined.sort_values('hair')
</code></pre><div><style scoped>.dataframe tbody tr th:only-of-type{vertical-align:middle}<pre class="mc-prism hide language-text"><code>.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</code></pre><p></style></p><table border=1 class=dataframe><thead><tr style=text-align:right><th></th><th>hair</th><th>col_0</th><th>col_1</th><th>col_2</th><th>col_3</th><th>col_4</th><th>col_5</th><th>col_6</th><th>col_7</th></tr></thead><tbody><tr><td>1</td><td>black</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>3</td><td>black</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>9</td><td>black</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>2</td><td>blonde</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>4</td><td>blonde</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>6</td><td>blonde</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>0</td><td>brown</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>5</td><td>brown</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>7</td><td>brown</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>8</td><td>brown</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div><p>Since the <u>number of dimensions defaults to 8</u>, the same amount of columns get created by the encoding. Since there are only three unique values in the <code>hair</code> column we see five columns with only zeros. <strong>Hashing gets interesing</strong> when we have a <u>lot of unique values</u> and the number of newly added columns should be smaller than the amount of unique values. In <em>Kaggle</em> competitions hashing has been a very sucessfull method for encoding high cardinality features.</p><br><h2 id=sum-encoding>Sum Encoding</h2><p>A <strong>Sum Encoder</strong> compares the mean of the dependent variable (<code>target</code>) for a given level of a categorical column to the overall mean of the target. This method is <u>very similar to One Hot Encoding</u> except that the number of created columns is always one less. This is the case because one <u>unique value is always held constant</u>.</p><pre class="mc-prism hide language-text"><code class=language-python>sum_enc = ce.SumEncoder(cols=['age'])
encoding = sum_enc.fit_transform(X['age'], y)
combined = pd.concat([X['age'], encoding, y], axis=1)
combined.sort_values('age')
</code></pre><div><style scoped>.dataframe tbody tr th:only-of-type{vertical-align:middle}<pre class="mc-prism hide language-text"><code>.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</code></pre><p></style></p><table border=1 class=dataframe><thead><tr style=text-align:right><th></th><th>age</th><th>intercept</th><th>age_0</th><th>age_1</th><th>age_2</th><th>age_3</th><th>target</th></tr></thead><tbody><tr><td>0</td><td>0-17</td><td>1</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0</td></tr><tr><td>1</td><td>18-29</td><td>1</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>1</td></tr><tr><td>5</td><td>18-29</td><td>1</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>1</td></tr><tr><td>9</td><td>18-29</td><td>1</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0</td></tr><tr><td>2</td><td>30-49</td><td>1</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0</td></tr><tr><td>6</td><td>30-49</td><td>1</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>1</td></tr><tr><td>7</td><td>30-49</td><td>1</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>1</td></tr><tr><td>3</td><td>50-69</td><td>1</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>1</td></tr><tr><td>8</td><td>50-69</td><td>1</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0</td></tr><tr><td>4</td><td>70+</td><td>1</td><td>-1.0</td><td>-1.0</td><td>-1.0</td><td>-1.0</td><td>0</td></tr></tbody></table></div><p>That one value is always contant can be observed in row 5 (index = 4), where the value for <em>70+</em> is always encoded as <em>-1</em> regardless of the column. Sum Encoding is is commonly used in Linear Regression (LR) types of models.</p><br><h2 id=target-encoding>Target Encoding</h2><p>Target Encoding or Mean Encoding directly correlates the encoded variable with a discrete target variable. So this approach can <u>only be used in classification problems</u>. The danger of this method lies within the <strong>problem of overfitting</strong> which can only be addressed by <em>regularization</em>. Nevertheless this Encoding technique has been very sucessfully used in <em>Kaggle</em> competitions.</p><pre class="mc-prism hide language-text"><code class=language-python>target_enc = ce.TargetEncoder(cols=['hair'])
encoding = target_enc.fit_transform(X['hair'], y).rename(columns={'hair':'hair_enc'})
combined = pd.concat([X['hair'], encoding, y], axis=1)
combined.sort_values('hair')
</code></pre><div><style scoped>.dataframe tbody tr th:only-of-type{vertical-align:middle}<pre class="mc-prism hide language-text"><code>.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</code></pre><p></style></p><table border=1 class=dataframe><thead><tr style=text-align:right><th></th><th>hair</th><th>hair_enc</th><th>target</th></tr></thead><tbody><tr><td>1</td><td>black</td><td>0.6468</td><td>1</td></tr><tr><td>3</td><td>black</td><td>0.6468</td><td>1</td></tr><tr><td>9</td><td>black</td><td>0.6468</td><td>0</td></tr><tr><td>2</td><td>blonde</td><td>0.3532</td><td>0</td></tr><tr><td>4</td><td>blonde</td><td>0.3532</td><td>0</td></tr><tr><td>6</td><td>blonde</td><td>0.3532</td><td>1</td></tr><tr><td>0</td><td>brown</td><td>0.5000</td><td>0</td></tr><tr><td>5</td><td>brown</td><td>0.5000</td><td>1</td></tr><tr><td>7</td><td>brown</td><td>0.5000</td><td>1</td></tr><tr><td>8</td><td>brown</td><td>0.5000</td><td>0</td></tr></tbody></table></div><p>As you can see there are only three different numeric values. Reason for this is that like in Frequence Encoding each unique category gets a new numerical value between 0 to 1. In the first row we can see that observations with the hair color <code>brown</code> are in 50% of the cases correlated to the target variable, hence the value 0.5.</p><br><h2 id=leave-one-out-encoding>Leave One Out Encoding</h2><p>Leave-one-out Encoding (<strong>LOO</strong> or <strong>LOOE</strong>) is another example of a target-based encoder. The name speaks for itself: we compute the <strong>mean target of category k</strong> for observation i <u>if observation i would be removed</u> from the dataset.</p><pre class="mc-prism hide language-text"><code class=language-python>loo = ce.LeaveOneOutEncoder(cols=['hair'])
encoding = loo.fit_transform(X['hair'], y).rename(columns={'hair':'hair_enc'})
combined = pd.concat([X['hair'], encoding, y], axis=1)
combined.sort_values('hair')
</code></pre><div><style scoped>.dataframe tbody tr th:only-of-type{vertical-align:middle}<pre class="mc-prism hide language-text"><code>.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</code></pre><p></style></p><table border=1 class=dataframe><thead><tr style=text-align:right><th></th><th>hair</th><th>hair_enc</th><th>target</th></tr></thead><tbody><tr><td>1</td><td>black</td><td>0.500000</td><td>1</td></tr><tr><td>3</td><td>black</td><td>0.500000</td><td>1</td></tr><tr><td>9</td><td>black</td><td>1.000000</td><td>0</td></tr><tr><td>2</td><td>blonde</td><td>0.500000</td><td>0</td></tr><tr><td>4</td><td>blonde</td><td>0.500000</td><td>0</td></tr><tr><td>6</td><td>blonde</td><td>0.000000</td><td>1</td></tr><tr><td>0</td><td>brown</td><td>0.666667</td><td>0</td></tr><tr><td>5</td><td>brown</td><td>0.333333</td><td>1</td></tr><tr><td>7</td><td>brown</td><td>0.333333</td><td>1</td></tr><tr><td>8</td><td>brown</td><td>0.666667</td><td>0</td></tr></tbody></table></div><p>There are other more complicated Target based Encoder like the <em>M-Estimate</em>, <em>Weight of Evidence</em> or <em>James-Steiner Encoder (WOE)</em> which also had their share of sucess in <em>Kaggle</em> competitions. If you&rsquo;re interested in those, check out this <a href=https://towardsdatascience.com/benchmarking-categorical-encoders-9c322bd77ee8>blog post</a> on Towards Data Science.</p><h1 id=summary>Summary</h1><p>Here a quick summary of all presented encoding techniques in a tabular format. This should help you and me to maintain a clear overview:</p><table class=mc-table><thead><tr><th>Technique</th><th>What does it do?</th><th>Variable Type</th></tr></thead><tbody><tr><td>One Hot Encoding</td><td>Each category gets its own column</td><td>Nominal, Ordinal</td></tr><tr><td>Ordinal Encoding</td><td>Each category gets mapped to an integer value</td><td>Ordinal</td></tr><tr><td>Binary Encoding</td><td>Each category gets mapped to a binary code and columns split according to the code length.</td><td>Ordinal</td></tr><tr><td>Frequency Encoding</td><td>The frequency of each value in category gets calculated and mapped</td><td>Nominal, Ordinal</td></tr><tr><td>Hashing Encoding</td><td>Replace values by hash code with varying length and columns split according to code length</td><td>Nominal, Ordinal</td></tr><tr><td>Sum Encoding</td><td>Similar to One Hot Encoding, except one less column created</td><td>Nominal, Ordinal</td></tr><tr><td>Target Encoding</td><td>Correlate variable to target variable</td><td>Nominal, Ordinal</td></tr><tr><td>Leave One Out Encoding</td><td>Similar to Target Encoding except current observation gets ignored in calculation</td><td>Nominal, Ordinal</td></tr></tbody></table><br><h1 id=final-discussion>Final Discussion</h1><p>Okay let&rsquo;s wrap this up by reflecting on the decision process when choosing an encoding method.</p><p><strong>Relevant questions</strong> to ask when you think about which Encoding strategy to choose are:</p><ul><li>What <u>variable type</u> do i have? Ordinal or Nominal</li><li><u>How many unique values</u> does the variable have (cardinality)? Low, Medium or High</li><li>What type of <u>Machine Learning problem</u> do i try to solve? For example: Classification, Regression, Clustering</li></ul><br><p>I can give you the following advise when choosing an technique to encode your data:</p><ul><li>First check the type of ML problem: Any kind of <strong>Target Encoding</strong> <u>only works for classification</u></li><li>In general the following Encodings make sense for <strong>ordinal features</strong>: <u>Ordinary, Binary, OneHot, Leave One Out, Target Encoding</u></li><li>If you have a <strong>ordinal columns with a lot of features</strong> (rare case) take a <u>Binary Encoder</u>. Leave One Out or Target Encoding also make sense.</li><li>In general the following Encodings make sense for <strong>nominal features</strong>: <u>OneHot, Hashing, LeaveOneOut, and Target encoding </u>. Avoid OneHot for high cardinality columns</li></ul><br><p>If you need a visual map to guide you through the decision, here is a flow chart i found:</p><p><a href=https://blog.featurelabs.com/encode-smarter/>source</a></p></article></div></div><footer><article>Copyright © 2023 by Marc-Philipp Esser</article></footer></main></body></html>